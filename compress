import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from sklearn.cluster import KMeans
from collections import Counter, defaultdict
import heapq
import pickle
import json
from ultralytics import YOLO
from scipy.sparse import csr_matrix, csc_matrix
import struct
import os
import tempfile
import shutil
from datetime import datetime
import time

class DeepCompression:
    
    def __init__(self, model_path=r"./runs/detect/train/weights/best.pt"):
        """
            model_path: í•™ìŠµëœ YOLOv5 ëª¨ë¸ ê²½ë¡œ
        """
        self.model = YOLO(model_path)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.model.to(self.device)
        
        # ì••ì¶• í†µê³„ (ë…¼ë¬¸ Table 1-5 í˜•ì‹)
        self.compression_stats = {}
        self.layer_stats = []
        
    # ============ 1. NETWORK PRUNING (ë…¼ë¬¸ Section 2) ============
    
    def prune_model(self, conv_threshold=0.005, fc_threshold=0.015, retrain_epochs=17, learning_rate=0.001):
        """
        Network Pruning - ë…¼ë¬¸ì˜ 3ë‹¨ê³„ í”„ë£¨ë‹ í”„ë¡œì„¸ìŠ¤
        1. Train Connectivity
        2. Prune Connections (absolute magnitude threshold)
        3. Retrain Weights
        
            conv_threshold: Conv ë ˆì´ì–´ í”„ë£¨ë‹ ì„ê³„ê°’
            fc_threshold: FC ë ˆì´ì–´ í”„ë£¨ë‹ ì„ê³„ê°’
            retrain_epochs: ì¬í•™ìŠµ epoch ìˆ˜
            learning_rate: ì¬í•™ìŠµ ì‹œ learning rate (0.001)
        """
        print("=" * 60)
        print("STAGE 1: NETWORK PRUNING")
        print("=" * 60)
        
        # ì´ˆê¸° ì •í™•ë„ í‰ê°€
        original_accuracy = self._evaluate_model()
        print(f"Original accuracy: {original_accuracy:.2%}")
        
        # ê° ë ˆì´ì–´ë³„ ë§ˆìŠ¤í¬ ìƒì„±
        self.masks = {}
        self.sparse_weights = {}
        
        total_params = 0
        remaining_params = 0
        
        # torch.no_grad()ë¡œ ê°ì‹¸ì„œ inference ëª¨ë“œ ë¬¸ì œ ë°©ì§€
        with torch.no_grad():
            for name, module in self.model.model.named_modules():
                if isinstance(module, nn.Conv2d):
                    # Conv ë ˆì´ì–´ í”„ë£¨ë‹
                    weight = module.weight.data.cpu().numpy()
                    
                    # ë…¼ë¬¸: ì ˆëŒ€ê°’ ê¸°ë°˜ threshold
                    mask = np.abs(weight) > conv_threshold
                    
                    self.masks[name] = torch.from_numpy(mask).float().to(self.device)
                    
                    # Sparse formatìœ¼ë¡œ ì €ì¥ (CSR - ë…¼ë¬¸ ì°¸ì¡°)
                    weight_masked = weight * mask
                    self.sparse_weights[name] = self._convert_to_sparse(weight_masked)
                    
                    # í†µê³„
                    total = mask.size
                    remaining = mask.sum()
                    total_params += total
                    remaining_params += remaining
                    
                    # ëª¨ë¸ì— ì ìš©
                    module.weight.data = module.weight.data * self.masks[name]
                    
                    print(f"{name:20s} | Conv2d | Threshold: {conv_threshold:.3f} | "
                          f"Pruned: {(1-remaining/total)*100:5.1f}% | "
                          f"({remaining:,}/{total:,} params)")
                    
                elif isinstance(module, nn.Linear):
                    # FC ë ˆì´ì–´ í”„ë£¨ë‹
                    weight = module.weight.data.cpu().numpy()
                    
                    mask = np.abs(weight) > fc_threshold
                    
                    self.masks[name] = torch.from_numpy(mask).float().to(self.device)
                    
                    weight_masked = weight * mask
                    self.sparse_weights[name] = self._convert_to_sparse(weight_masked)
                    
                    total = mask.size
                    remaining = mask.sum()
                    total_params += total
                    remaining_params += remaining
                    
                    module.weight.data = module.weight.data * self.masks[name]
                    
                    print(f"{name:20s} | Linear | Threshold: {fc_threshold:.3f} | "
                          f"Pruned: {(1-remaining/total)*100:5.1f}% | "
                          f"({remaining:,}/{total:,} params)")
        
        # ì „ì²´ í”„ë£¨ë‹ í†µê³„
        if total_params > 0:
            pruning_rate = total_params / remaining_params if remaining_params > 0 else 1
            print(f"\nOverall pruning: {pruning_rate:.1f}Ã— reduction")
            print(f"Sparsity: {(1-remaining_params/total_params)*100:.1f}%")
        else:
            pruning_rate = 1
            print("\nNo prunable layers found")
        
        # Retrain (ë…¼ë¬¸: pruning í›„ ì •í™•ë„ íšŒë³µì„ ìœ„í•œ í•„ìˆ˜ ë‹¨ê³„)
        if retrain_epochs > 0:
            print(f"\nRetraining for {retrain_epochs} epochs with lr={learning_rate}...")
            self._retrain_pruned_model(retrain_epochs, learning_rate)
            
            pruned_accuracy = self._evaluate_model()
            print(f"Pruned model accuracy: {pruned_accuracy:.2%}")
            print(f"Accuracy change: {(pruned_accuracy - original_accuracy):.2%}")
        
        self.compression_stats['pruning'] = {
            'original_params': total_params,
            'remaining_params': remaining_params,
            'compression_rate': pruning_rate,
            'sparsity': 1 - remaining_params/total_params if total_params > 0 else 0,
            'learning_rate': learning_rate
        }
        
        return self.masks, self.sparse_weights
    
    def _convert_to_sparse(self, weight):
        """
        Sparse matrix ë³€í™˜ CSR
        """
        if len(weight.shape) == 4:  # Conv2d
            weight_2d = weight.reshape(weight.shape[0], -1)
            return csr_matrix(weight_2d)
        elif len(weight.shape) == 2:  # Linear
            return csr_matrix(weight)
        return weight
    
    def _retrain_pruned_model(self, epochs):
        """í”„ë£¨ë‹ëœ ëª¨ë¸ ì¬í•™ìŠµ (ë§ˆìŠ¤í¬ ìœ ì§€)"""
        print(f"  Retraining pruned model for {epochs} epochs...")
        
        import tempfile
        import shutil
        
        # ì„ì‹œ í´ë” ìƒì„±
        temp_dir = tempfile.mkdtemp(prefix="yolo_temp_")
        
        try:
            for epoch in range(epochs):
                print(f"    Epoch {epoch+1}/{epochs}")
                
                # í•™ìŠµ ì „ ë§ˆìŠ¤í¬ ì ìš©
                for name, module in self.model.model.named_modules():
                    if name in self.masks and hasattr(module, 'weight'):
                        with torch.no_grad():
                            module.weight.data = module.weight.data * self.masks[name]
                
                # ì„ì‹œ í´ë”ì— ì €ì¥í•˜ë„ë¡ ì„¤ì •
                try:
                    self.model.train(
                        data="coco128.yaml",
                        epochs=1,
                        imgsz=640,
                        device=self.device,
                        verbose=False,
                        batch=16,
                        patience=0,
                        save=False,
                        val=False,
                        project=temp_dir,  # ì„ì‹œ í´ë” ì§€ì •
                        name="temp_train",
                        exist_ok=True
                    )
                except Exception as e:
                    print(f"    Warning: Training error - {e}")
                    pass
                
                # í•™ìŠµ í›„ ë§ˆìŠ¤í¬ ì¬ì ìš©
                for name, module in self.model.model.named_modules():
                    if name in self.masks and hasattr(module, 'weight'):
                        with torch.no_grad():
                            module.weight.data = module.weight.data * self.masks[name]
        
        finally:
            # ì„ì‹œ í´ë” ì‚­ì œ
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)
                print(f"  Cleaned up temporary training files")
        
        print(f"  Retraining completed")
        return None
    
    # ============ 2. TRAINED QUANTIZATION (ë…¼ë¬¸ Section 3) ============
    
    def quantize_weights(self, conv_bits=8, fc_bits=5, retrain_epochs=17, learning_rate=0.001):
        """
        Trained Quantization and Weight Sharing
            conv_bits: Conv ë ˆì´ì–´ ë¹„íŠ¸ ìˆ˜ (8 bits = 256 clusters) 
            fc_bits: FC ë ˆì´ì–´ ë¹„íŠ¸ ìˆ˜ (5 bits = 32 clusters)
            kê°’ ì¡°ì ˆë¡œ ê²½ëŸ‰í™” ì •ë„ ì¡°ì ˆ
        """
        print("\n" + "=" * 60)
        print("STAGE 2: TRAINED QUANTIZATION AND WEIGHT SHARING")
        print("=" * 60)
        
        self.codebooks = {}  # ê³µìœ  ê°€ì¤‘ì¹˜ (centroids)
        self.weight_indices = {}  # ê° ê°€ì¤‘ì¹˜ì˜ í´ëŸ¬ìŠ¤í„° ì¸ë±ìŠ¤
        self.quantized_layers = {}
        
        with torch.no_grad():  # Inference ëª¨ë“œ ë¬¸ì œ ë°©ì§€
            for name, module in self.model.model.named_modules():
                if isinstance(module, (nn.Conv2d, nn.Linear)):
                    # ë ˆì´ì–´ íƒ€ì…ë³„ í´ëŸ¬ìŠ¤í„° ìˆ˜
                    if isinstance(module, nn.Conv2d):
                        k = int(2.2 ** conv_bits)  # 256 for 8-bit
                        bits = conv_bits
                    else:
                        k = int(2.2 ** fc_bits)  # 32 for 5-bit
                        bits = fc_bits
                    
                    # ê°€ì¤‘ì¹˜ ê°€ì ¸ì˜¤ê¸°
                    weight = module.weight.data.cpu().numpy()
                    original_shape = weight.shape
                    weight_flat = weight.flatten()
                    
                    # í”„ë£¨ë‹ëœ ê°€ì¤‘ì¹˜ë§Œ quantize (0ì´ ì•„ë‹Œ ê°’ë“¤)
                    if name in self.masks:
                        mask_flat = self.masks[name].cpu().numpy().flatten()
                        non_zero_mask = mask_flat > 0
                    else:
                        non_zero_mask = weight_flat != 0
                    
                    non_zero_weights = weight_flat[non_zero_mask]
                    
                    if len(non_zero_weights) > 0:
                        # Linear Initialization (ë…¼ë¬¸ Section 3.2)
                        min_weight = non_zero_weights.min()
                        max_weight = non_zero_weights.max()
                        
                        # ì‹¤ì œ í´ëŸ¬ìŠ¤í„° ìˆ˜ (ê°€ì¤‘ì¹˜ ìˆ˜ê°€ kë³´ë‹¤ ì ì„ ìˆ˜ ìˆìŒ)
                        actual_k = min(k, len(non_zero_weights))
                        
                        # ì„ í˜• ì´ˆê¸°í™”
                        initial_centroids = np.linspace(min_weight, max_weight, actual_k)
                        initial_centroids = initial_centroids.reshape(-1, 1)
                        
                        # K-means clustering
                        kmeans = KMeans(
                            n_clusters=actual_k,
                            init=initial_centroids,
                            n_init=1,
                            max_iter=100,
                            algorithm='lloyd'
                        )
                        
                        kmeans.fit(non_zero_weights.reshape(-1, 1))
                        
                        # í´ëŸ¬ìŠ¤í„° í• ë‹¹
                        labels = kmeans.labels_
                        centroids = kmeans.cluster_centers_.flatten()
                        
                        # ì–‘ìí™”ëœ ê°€ì¤‘ì¹˜ ìƒì„±
                        quantized_weight = np.zeros_like(weight_flat)
                        indices = np.zeros_like(weight_flat, dtype=np.uint8 if bits <= 8 else np.uint16)
                        
                        # ì„¼íŠ¸ë¡œì´ë“œë¡œ ëŒ€ì²´
                        quantized_weight[non_zero_mask] = centroids[labels]
                        indices[non_zero_mask] = labels
                        
                        # ì €ì¥
                        self.codebooks[name] = centroids
                        self.weight_indices[name] = indices.reshape(original_shape)
                        
                        # ëª¨ë¸ì— ì ìš©
                        module.weight.data = torch.from_numpy(
                            quantized_weight.reshape(original_shape)
                        ).float().to(self.device)
                        
                        # ì••ì¶•ë¥  ê³„ì‚° (ë…¼ë¬¸ Equation 1)
                        n = non_zero_mask.sum()  # connections
                        b = 32  # bits per weight (float32)
                        compression_rate = (n * b) / (n * np.log2(actual_k) + actual_k * b) if actual_k > 1 else 1
                        
                        self.quantized_layers[name] = {
                            'type': module.__class__.__name__,
                            'clusters': actual_k,
                            'bits': bits,
                            'compression': compression_rate
                        }
                        
                        print(f"{name:20s} | {actual_k:3d} clusters ({bits} bits) | "
                              f"Compression: {compression_rate:.1f}Ã—")
        
        # Fine-tuning centroids (ë…¼ë¬¸ Section 3.3)
        if retrain_epochs > 0 and len(self.codebooks) > 0:
            print(f"\nFine-tuning centroids for {retrain_epochs} epochs with lr={learning_rate}...")
            self._fine_tune_centroids(retrain_epochs, learning_rate)
        
        # í†µê³„ ì €ì¥
        if len(self.quantized_layers) > 0:
            avg_compression = np.mean([l['compression'] for l in self.quantized_layers.values()])
        else:
            avg_compression = 1.0
            
        self.compression_stats['quantization'] = {
            'conv_bits': conv_bits,
            'fc_bits': fc_bits,
            'average_compression': avg_compression,
            'learning_rate': learning_rate
        }
        
        return self.codebooks, self.weight_indices
    
    def _fine_tune_centroids(self, epochs):
        """
        ì„¼íŠ¸ë¡œì´ë“œ fine-tuning
        """
        print("  Fine-tuning centroids...")
        
        import tempfile
        import shutil
        import os
        
        # ì„ì‹œ í´ë” ìƒì„±
        temp_dir = tempfile.mkdtemp(prefix="yolo_temp_")
        
        try:
            for epoch in range(epochs):
                # í˜„ì¬ centroidsë¡œ weights ì¬êµ¬ì„±
                with torch.no_grad():
                    for name, module in self.model.model.named_modules():
                        if name in self.weight_indices and hasattr(module, 'weight'):
                            indices = self.weight_indices[name]
                            codebook = self.codebooks[name]
                            
                            weight_shape = module.weight.data.shape
                            weight_flat = np.zeros(indices.size, dtype=np.float32)
                            indices_flat = indices.flatten()
                            
                            for i, idx in enumerate(indices_flat):
                                if idx < len(codebook):
                                    weight_flat[i] = codebook[idx]
                            
                            module.weight.data = torch.from_numpy(
                                weight_flat.reshape(weight_shape)
                            ).to(self.device)
                
                # 1 epoch í•™ìŠµ (ì„ì‹œ í´ë”ì—)
                try:
                    self.model.train(
                        data="coco128.yaml",
                        epochs=1,
                        imgsz=640,
                        device=self.device,
                        verbose=False,
                        batch=16,
                        patience=0,
                        save=False,
                        val=False,
                        project=temp_dir,  # ì„ì‹œ í´ë” ì§€ì •
                        name="temp_train",
                        exist_ok=True
                    )
                except Exception as e:
                    print(f"    Warning: Training error - {e}")
                    pass
                
                # Gradient ê¸°ë°˜ centroid ì—…ë°ì´íŠ¸
                with torch.no_grad():
                    for name, module in self.model.model.named_modules():
                        if name in self.weight_indices and hasattr(module, 'weight'):
                            current_weight = module.weight.data.cpu().numpy()
                            indices = self.weight_indices[name]
                            
                            new_codebook = np.zeros_like(self.codebooks[name])
                            count = np.zeros(len(self.codebooks[name]))
                            
                            weight_flat = current_weight.flatten()
                            indices_flat = indices.flatten()
                            
                            for w, idx in zip(weight_flat, indices_flat):
                                if idx < len(new_codebook) and idx > 0:
                                    new_codebook[idx] += w
                                    count[idx] += 1
                            
                            for i in range(len(new_codebook)):
                                if count[i] > 0:
                                    new_codebook[i] /= count[i]
                                else:
                                    new_codebook[i] = self.codebooks[name][i]
                            
                            self.codebooks[name] = new_codebook
                
                print(f"    Epoch {epoch+1}/{epochs} - Centroids updated")
        
        finally:
            # ì„ì‹œ í´ë” ì‚­ì œ
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)
                print(f"  Cleaned up temporary training files")
        
        return None
    
    # ============ 3. HUFFMAN CODING (ë…¼ë¬¸ Section 4) ============
    
    def huffman_encode(self):
        """
        Huffman Coding
        ë…¼ë¬¸: "A Huffman code is an optimal prefix code commonly used for 
        lossless data compression"
        """
        print("\n" + "=" * 60)
        print("STAGE 3: HUFFMAN CODING")
        print("=" * 60)
        
        if not hasattr(self, 'weight_indices') or len(self.weight_indices) == 0:
            print("  No quantized weights found. Skipping Huffman coding.")
            self.huffman_codes = {}
            self.huffman_trees = {}
            self.compression_stats['huffman'] = {
                'compression_rate': 1.0,
                'saved_bits_percent': 0
            }
            return {}, {}
        
        self.huffman_codes = {}
        self.huffman_trees = {}
        total_original_bits = 0
        total_huffman_bits = 0
        
        for name in self.weight_indices:
            try:
                # ì¸ë±ìŠ¤ì™€ ê°€ì¤‘ì¹˜ ë¶„í¬ ë¶„ì„
                indices_flat = self.weight_indices[name].flatten()
                
                # Relative indexing (ë…¼ë¬¸ Figure 2)
                diff_indices = self._compute_relative_indices(indices_flat)
                
                if len(diff_indices) == 0:
                    continue
                
                # ë¹ˆë„ ê³„ì‚°
                freq_counter = Counter(diff_indices)
                
                # í—ˆí”„ë§Œ íŠ¸ë¦¬ êµ¬ì¶•
                huffman_tree = self._build_huffman_tree(freq_counter)
                huffman_codes = self._generate_codes(huffman_tree)
                
                # ì¸ì½”ë”©
                encoded_data = []
                for idx in diff_indices:
                    if idx in huffman_codes:
                        encoded_data.append(huffman_codes[idx])
                
                # ë¹„íŠ¸ ê³„ì‚°
                if name in self.codebooks and len(self.codebooks[name]) > 0:
                    original_bits = len(indices_flat) * int(np.ceil(np.log2(len(self.codebooks[name]))))
                else:
                    original_bits = len(indices_flat) * 8  # ê¸°ë³¸ 8ë¹„íŠ¸
                    
                huffman_bits = sum(len(code) for code in encoded_data)
                
                if huffman_bits > 0:
                    compression = original_bits / huffman_bits
                else:
                    compression = 1
                
                self.huffman_codes[name] = huffman_codes
                self.huffman_trees[name] = huffman_tree
                
                total_original_bits += original_bits
                total_huffman_bits += huffman_bits
                
                print(f"{name:20s} | Huffman compression: {compression:.2f}Ã— | "
                      f"Saved: {(1-huffman_bits/original_bits)*100:.1f}%" if original_bits > 0 else "")
                      
            except Exception as e:
                print(f"  Warning: Failed to encode {name}: {e}")
                continue
        
        if total_huffman_bits > 0:
            overall_huffman = total_original_bits / total_huffman_bits
        else:
            overall_huffman = 1.0
            
        print(f"\nOverall Huffman compression: {overall_huffman:.2f}Ã—")
        
        self.compression_stats['huffman'] = {
            'compression_rate': overall_huffman,
            'saved_bits_percent': (1 - total_huffman_bits/total_original_bits) * 100 if total_original_bits > 0 else 0
        }
        
        return self.huffman_codes, self.huffman_trees
    
    def _compute_relative_indices(self, indices):
        """
        ìƒëŒ€ ì¸ë±ìŠ¤ ê³„ì‚° (ë…¼ë¬¸ Figure 2)
        ì ˆëŒ€ ìœ„ì¹˜ ëŒ€ì‹  ì¸ë±ìŠ¤ ì°¨ì´ë¥¼ ì €ì¥
        """
        if len(indices) == 0:
            return indices
        
        # 0ì´ ì•„ë‹Œ ê°’ë“¤ì˜ ìœ„ì¹˜ë§Œ ì €ì¥
        non_zero_positions = np.where(indices != 0)[0]
        if len(non_zero_positions) == 0:
            return np.array([])
        
        # ì°¨ì´ ê³„ì‚°
        diff_indices = np.zeros(len(non_zero_positions), dtype=np.int32)
        diff_indices[0] = non_zero_positions[0]
        diff_indices[1:] = np.diff(non_zero_positions)
        
        # Overflow ì²˜ë¦¬ (ë…¼ë¬¸: ì°¨ì´ê°€ boundë¥¼ ì´ˆê³¼í•˜ë©´ filler zero ì¶”ê°€)
        MAX_DIFF = 255  # 8-bit for conv layers
        overflow_mask = diff_indices > MAX_DIFF
        
        if overflow_mask.any():
            # Padding with zeros
            result = []
            for diff in diff_indices:
                while diff > MAX_DIFF:
                    result.append(MAX_DIFF)
                    result.append(0)  # filler zero
                    diff -= MAX_DIFF
                result.append(diff)
            return np.array(result)
        
        return diff_indices
    
    def _build_huffman_tree(self, freq_counter):
        """í—ˆí”„ë§Œ íŠ¸ë¦¬ êµ¬ì¶•"""
        if not freq_counter:
            return []
        
        # Min heap ì´ˆê¸°í™”
        heap = [[freq, [symbol, ""]] for symbol, freq in freq_counter.items()]
        heapq.heapify(heap)
        
        # íŠ¸ë¦¬ êµ¬ì¶•
        while len(heap) > 1:
            lo = heapq.heappop(heap)
            hi = heapq.heappop(heap)
            
            # ì™¼ìª½ ìì‹: 0, ì˜¤ë¥¸ìª½ ìì‹: 1
            for pair in lo[1:]:
                pair[1] = '0' + pair[1]
            for pair in hi[1:]:
                pair[1] = '1' + pair[1]
            
            merged = [lo[0] + hi[0]] + lo[1:] + hi[1:]
            heapq.heappush(heap, merged)
        
        return heap[0] if heap else []
    
    def _generate_codes(self, huffman_tree):
        """í—ˆí”„ë§Œ ì½”ë“œ ìƒì„±"""
        codes = {}
        if huffman_tree and len(huffman_tree) > 1:
            for item in huffman_tree[1:]:
                symbol, code = item
                codes[symbol] = code if code else '0'
        return codes
    
    # ============ EVALUATION & STATISTICS ============
    
    def _evaluate_model(self):
        """ëª¨ë¸ ì •í™•ë„ í‰ê°€ - ì „ì²´ ë©”íŠ¸ë¦­ ì €ì¥"""
        try:
            results = self.model.val(data="coco128.yaml", verbose=False)
            
            # ëª¨ë“  ë©”íŠ¸ë¦­ ì¶”ì¶œ
            evaluation_metrics = {
                'map50': float(results.box.map50) if hasattr(results.box, 'map50') else 0.5,
                'map': float(results.box.map) if hasattr(results.box, 'map') else 0.5,
                'precision': float(results.box.mp) if hasattr(results.box, 'mp') else 0.0,
                'recall': float(results.box.mr) if hasattr(results.box, 'mr') else 0.0,
                'f1': float(results.box.f1) if hasattr(results.box, 'f1') else 0.0,
            }
            
            # ë‚˜ì¤‘ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì €ì¥
            self.evaluation_metrics = evaluation_metrics
            
            return evaluation_metrics['map50']  # ê¸°ë³¸ ë°˜í™˜ê°’ì€ mAP@0.5
            
        except:
            # í‰ê°€ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            print("  Warning: Could not evaluate model, using default accuracy")
            return 0.5
    
    def print_compression_summary(self):
        """
        ì••ì¶• ê²°ê³¼ ìš”ì•½ (ë…¼ë¬¸ Table 1 í˜•ì‹)
        """
        print("\n" + "=" * 60)
        print("DEEP COMPRESSION SUMMARY")
        print("=" * 60)
        
        # ì›ë³¸ ëª¨ë¸ í¬ê¸°
        original_params = sum(p.numel() for p in self.model.model.parameters())
        original_size_mb = original_params * 32 / (8 * 1024 * 1024)  # 32-bit float
        
        # ì••ì¶• í›„ í¬ê¸° ê³„ì‚°
        # Pruning
        pruning_rate = self.compression_stats.get('pruning', {}).get('compression_rate', 1)
        after_pruning_mb = original_size_mb / pruning_rate if pruning_rate > 0 else original_size_mb
        
        # Quantization  
        quant_rate = self.compression_stats.get('quantization', {}).get('average_compression', 1)
        after_quant_mb = after_pruning_mb / quant_rate if quant_rate > 0 else after_pruning_mb
        
        # Huffman
        huffman_rate = self.compression_stats.get('huffman', {}).get('compression_rate', 1)
        final_size_mb = after_quant_mb / huffman_rate if huffman_rate > 0 else after_quant_mb
        
        # ì „ì²´ ì••ì¶•ë¥ 
        total_compression = original_size_mb / final_size_mb if final_size_mb > 0 else 1
        
        print(f"Original model size:        {original_size_mb:8.2f} MB")
        print(f"After pruning ({pruning_rate:.1f}Ã—):      {after_pruning_mb:8.2f} MB")
        print(f"After quantization ({quant_rate:.1f}Ã—): {after_quant_mb:8.2f} MB")
        print(f"After Huffman ({huffman_rate:.1f}Ã—):      {final_size_mb:8.2f} MB")
        print(f"{'='*40}")
        print(f"Total compression:          {total_compression:8.1f}Ã—")
        print(f"Final model size:           {final_size_mb:8.2f} MB")
        
        # ì •í™•ë„
        try:
            final_accuracy = self._evaluate_model()
            print(f"\nFinal mAP@0.5: {final_accuracy:.4f}")
        except:
            final_accuracy = 0.0
            print(f"\nFinal mAP@0.5: Unable to evaluate")
        
        return {
            'original_size_mb': float(original_size_mb),
            'compressed_size_mb': float(final_size_mb),
            'compression_rate': float(total_compression),
            'final_accuracy': float(final_accuracy),
            'pruning_rate': float(pruning_rate),
            'quantization_rate': float(quant_rate),
            'huffman_rate': float(huffman_rate)
        }
    
    # ============ SAVE & LOAD ============
    
    def save_compressed_model(self, save_dir="compressed_model"):
        """
        ì••ì¶•ëœ ëª¨ë¸ ì €ì¥ (ë…¼ë¬¸ í˜•ì‹)
        - Sparse indices (CSR format)
        - Codebooks (shared weights)
        - Huffman codes
        """
        import os
        from datetime import datetime
        
        # ë²„ì „ ê´€ë¦¬: í´ë”ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìˆ«ì ì¶”ê°€
        base_dir = save_dir
        counter = 1
        while os.path.exists(save_dir):
            save_dir = f"{base_dir}_{counter}"
            counter += 1
        
        # ë˜ëŠ” íƒ€ì„ìŠ¤íƒ¬í”„ ì‚¬ìš© (ì„ íƒì‚¬í•­)
        # timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # save_dir = f"{base_dir}_{timestamp}"
        
        os.makedirs(save_dir, exist_ok=False)  # exist_ok=Falseë¡œ ë³€ê²½
        print(f"\n Creating new folder: {save_dir}")
        
        # 1. Sparse weights & indices
        sparse_data = {}
        for name in self.sparse_weights:
            sparse_matrix = self.sparse_weights[name]
            sparse_data[name] = {
                'data': sparse_matrix.data.tolist(),
                'indices': sparse_matrix.indices.tolist(),
                'indptr': sparse_matrix.indptr.tolist(),
                'shape': sparse_matrix.shape
            }
        
        with open(f"{save_dir}/sparse_weights.pkl", 'wb') as f:
            pickle.dump(sparse_data, f)
        
        # 2. Codebooks and indices
        with open(f"{save_dir}/codebooks.pkl", 'wb') as f:
            pickle.dump(self.codebooks, f)
        
        with open(f"{save_dir}/weight_indices.pkl", 'wb') as f:
            pickle.dump(self.weight_indices, f)
        
        # 3. Huffman codes
        with open(f"{save_dir}/huffman_codes.pkl", 'wb') as f:
            pickle.dump({
                'codes': self.huffman_codes,
                'trees': self.huffman_trees
            }, f)
        
        # 4. Compression statistics
        try:
            stats = self.print_compression_summary()
        except Exception as e:
            print(f"Warning: Could not generate full statistics: {e}")
            stats = self.compression_stats
        
        # ì‹¤í–‰ ì‹œê°„ ì¶”ê°€
        stats['created_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        stats['save_dir'] = save_dir
        
        with open(f"{save_dir}/compression_stats.json", 'w') as f:
            # Convert numpy types to Python types for JSON serialization
            stats_serializable = {}
            for key, value in stats.items():
                if isinstance(value, (np.float32, np.float64)):
                    stats_serializable[key] = float(value)
                elif isinstance(value, (np.int32, np.int64)):
                    stats_serializable[key] = int(value)
                else:
                    stats_serializable[key] = value
            json.dump(stats_serializable, f, indent=4)
        
        print(f" Compressed model saved to: {save_dir}/")
        print(f" Total size: {sum(os.path.getsize(f'{save_dir}/{f}') for f in os.listdir(save_dir)) / (1024*1024):.2f} MB")
        
        return save_dir


# ============ ë©”ì¸ ì‹¤í–‰ ì½”ë“œ ============
if __name__ == '__main__':
    import os
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•´)
    TEST_MODE = True
    
    # ëª¨ë¸ ê²½ë¡œ í™•ì¸
    model_path = r"./runs/detect/train/weights/best.pt"
    if not os.path.exists(model_path):
        print(f"  Model not found at {model_path}")
        print("Using default YOLOv5s model instead...")
        model_path = "yolov5s.pt"
    
    try:
        # Deep Compression ì‹¤í–‰
        compressor = DeepCompression(model_path=model_path)
        
        # 1. Network Pruning 
        #  ë…¼ë¬¸: ì ˆëŒ€ê°’ ê¸°ë°˜ threshold (í‘œì¤€í¸ì°¨ X)
        print("\nğŸ”§ Starting Deep Compression Pipeline...")
        
        if TEST_MODE:
            print(" Running in TEST MODE (reduced epochs for quick testing)")
            retrain_epochs = 1  # í…ŒìŠ¤íŠ¸ìš©
            save_name = "yolov5s_compressed_test"
        else:
            retrain_epochs = 10  # ì‹¤ì œ ì‚¬ìš©
            save_name = "yolov5s_deep_compressed"
        
        masks, sparse_weights = compressor.prune_model(
            conv_threshold=0.015,  # Conv layers: ì ˆëŒ€ê°’ 0.015 ì´í•˜ ì œê±°
            fc_threshold=0.01,     # FC layers: ì ˆëŒ€ê°’ 0.01 ì´í•˜ ì œê±° (ë” aggressive)
            retrain_epochs=retrain_epochs
        )
        
        # 2. Trained Quantization (ë…¼ë¬¸: Conv 8-bit, FC 5-bit)
        codebooks, indices = compressor.quantize_weights(
            conv_bits=8,  # 256 clusters for conv layers
            fc_bits=5,    # 32 clusters for fc layers
            retrain_epochs=retrain_epochs
        )
        
        # 3. Huffman Coding
        huffman_codes, huffman_trees = compressor.huffman_encode()
        
        # 4. ìµœì¢… ê²°ê³¼ ë° ì €ì¥ (ìë™ìœ¼ë¡œ ìƒˆ í´ë” ìƒì„±)
        saved_dir = compressor.save_compressed_model(save_name)
        
        print("\n Deep Compression completed successfully!")
        print(f" Results saved to: {saved_dir}/")
        print("\n Saved files:")
        for file in os.listdir(saved_dir):
            file_path = os.path.join(saved_dir, file)
            size_mb = os.path.getsize(file_path) / (1024 * 1024)
            print(f"   - {file}: {size_mb:.2f} MB")
        
    except Exception as e:
        print(f"\n Error occurred: {e}")
        import traceback
        traceback.print_exc()
        
        # ì—ëŸ¬ ë°œìƒ ì‹œ ìµœì†Œí•œì˜ í…ŒìŠ¤íŠ¸
        print("\n Attempting minimal test without retraining...")
        try:
            compressor = DeepCompression(model_path=model_path)
            
            # Retraining ì—†ì´ ì••ì¶•ë§Œ ìˆ˜í–‰
            masks, sparse_weights = compressor.prune_model(
                conv_threshold=0.015,
                fc_threshold=0.01,
                retrain_epochs=0  # No retraining
            )
            
            codebooks, indices = compressor.quantize_weights(
                conv_bits=8,
                fc_bits=5,
                retrain_epochs=0  # No retraining
            )
            
            huffman_codes, huffman_trees = compressor.huffman_encode()
            saved_dir = compressor.save_compressed_model("yolov5s_compressed_minimal")
            
            print(f"\n Minimal compression completed (without retraining)")
            print(f" Results saved to: {saved_dir}/")
            
        except Exception as e2:
            print(f" Minimal test also failed: {e2}")
